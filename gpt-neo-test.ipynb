{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b1a438-cf1d-402e-a56f-2c4f9dd5ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, FlaxGPTNeoForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d3c070-a440-441d-a7d8-941a864b0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxAutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9734882-f43e-4cc4-b6c2-8cabf3aa8566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mFlaxAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.\n",
       "\n",
       "Note:\n",
       "    Loading a model from its configuration file does **not** load the model weights. It only affects the\n",
       "    model's configuration. Use :meth:`~transformers.FlaxAutoModelForCausalLM.from_pretrained` to load the model\n",
       "    weights.\n",
       "\n",
       "Args:\n",
       "    config (:class:`~transformers.PretrainedConfig`):\n",
       "        The model class to instantiate is selected based on the configuration class:\n",
       "\n",
       "        - :class:`~transformers.GPT2Config` configuration class: :class:`~transformers.FlaxGPT2LMHeadModel` (OpenAI GPT-2 model)\n",
       "        - :class:`~transformers.GPTNeoConfig` configuration class: :class:`~transformers.FlaxGPTNeoForCausalLM` (GPT Neo model)\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> from transformers import AutoConfig, FlaxAutoModelForCausalLM\n",
       "    >>> # Download configuration from huggingface.co and cache.\n",
       "    >>> config = AutoConfig.from_pretrained('bert-base-cased')\n",
       "    >>> model = FlaxAutoModelForCausalLM.from_config(config)\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/transformers/src/transformers/models/auto/auto_factory.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FlaxAutoModelForCausalLM.from_config??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d50cd18-33ed-4b67-82ad-5c48eb9a9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'EleutherAI/gpt-neo-125M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96394e7c-15a9-4464-9775-7bc9ddf2e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f391f53-fe70-46e6-a5de-4f4b479b86e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoConfig {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0,\n",
       "  \"attention_layers\": [\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\"\n",
       "  ],\n",
       "  \"attention_types\": [\n",
       "    [\n",
       "      [\n",
       "        \"global\",\n",
       "        \"local\"\n",
       "      ],\n",
       "      6\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embed_dropout\": 0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": null,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neo\",\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"resid_dropout\": 0,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.9.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257,\n",
       "  \"window_size\": 256\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoConfig.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "065c03c3-2e4a-4f20-a30d-25ada1418b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed89aae89749aba08b715ec2258b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = FlaxGPTNeoForCausalLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2f9fb26-2e26-4f57-aa93-e349475203f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75c0c2f6-47ad-41c3-8c66-a1ceeecde061",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "666977a1-de0d-4900-bf61-ae2b672e51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='jax')\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "249e4a8a-7a7e-4e8b-83be-7184a4c0dd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19, 50257)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "566d8be0-26a3-4429-ac95-b88b9de951a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
      "        and, multinomial sampling.\n",
      "\n",
      "        Apart from :obj:`input_ids`, all the arguments below will default to the value of the attribute of the same\n",
      "        name inside the :class:`~transformers.PretrainedConfig` of the model. The default values indicated are the\n",
      "        default values of those config.\n",
      "\n",
      "        Most of these parameters are explained in more detail in `this blog post\n",
      "        <https://huggingface.co/blog/how-to-generate>`__.\n",
      "\n",
      "        Parameters:\n",
      "\n",
      "            input_ids (:obj:`jax_xla.DeviceArray` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "                The sequence used as a prompt for the generation.\n",
      "            max_length (:obj:`int`, `optional`, defaults to 20):\n",
      "                The maximum length of the sequence to be generated.\n",
      "            do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "            temperature (:obj:`float`, `optional`, defaults to 1.0):\n",
      "                The value used to module the next token probabilities.\n",
      "            top_k (:obj:`int`, `optional`, defaults to 50):\n",
      "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "            top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
      "                If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
      "                higher are kept for generation.\n",
      "            pad_token_id (:obj:`int`, `optional`):\n",
      "                The id of the `padding` token.\n",
      "            bos_token_id (:obj:`int`, `optional`):\n",
      "                The id of the `beginning-of-sequence` token.\n",
      "            eos_token_id (:obj:`int`, `optional`):\n",
      "                The id of the `end-of-sequence` token.\n",
      "            num_beams (:obj:`int`, `optional`, defaults to 1):\n",
      "                Number of beams for beam search. 1 means no beam search.\n",
      "            decoder_start_token_id (:obj:`int`, `optional`):\n",
      "                If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
      "            trace (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "                Whether to trace generation. Setting ``trace=False`` should only be used for debugging and will lead to\n",
      "                a considerably slower runtime.\n",
      "            params (:obj:`Dict[str, jax_xla.DeviceArray]`, `optional`):\n",
      "                Optionally the model parameters can be passed. Can be useful for parallelized generation.\n",
      "            model_kwargs:\n",
      "                Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model.\n",
      "\n",
      "        Return:\n",
      "            :class:`~transformers.file_utils.ModelOutput`.\n",
      "\n",
      "        Examples::\n",
      "            >>> from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
      "\n",
      "            >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "            >>> model = FlaxAutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "            >>> input_context = \"The dog\"\n",
      "            >>> # encode input context\n",
      "            >>> input_ids = tokenizer(input_context, return_tensors=\"jax\").input_ids\n",
      "            >>> # generate candidates using sampling\n",
      "            >>> outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)\n",
      "            >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(model.generate.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eee873f5-073c-4cbe-8b15-114ea18b2de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[  198, 11748, 28034,   198,  6738, 28034,  1330,   299,\n",
       "                 77,   198,   198,  4871,  9104,     7, 20471,    13,\n",
       "              26796,  2599,   198]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82666225-3ab7-405f-9536-4e9e3085be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(input_ids,\n",
    "                     max_length=200, \n",
    "#                      num_beams=5,\n",
    "                     pad_token_id = tokenizer.pad_token_id\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6cc862b-23ef-417d-ae83-1b2eafb0460f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlaxGreedySearchOutput(sequences=DeviceArray([[  198, 11748, 28034,   198,  6738, 28034,  1330,   299,\n",
       "                 77,   198,   198,  4871,  9104,     7, 20471,    13,\n",
       "              26796,  2599,   198,   220,   220,   220,   825, 11593,\n",
       "              15003,   834,     7,   944,    11,  1438,    11,  2746,\n",
       "                 11, 12429, 46265, 22046,  2599,   198,   220,   220,\n",
       "                220,   220,   220,   220,   220,  2208,     7, 17633,\n",
       "                 11,  2116,   737,   834, 15003,   834,     7,  3672,\n",
       "                 11,  2746,    11, 12429, 46265, 22046,     8,   198,\n",
       "                220,   220,   220,   220,   220,   220,   220,  2116,\n",
       "                 13,  3672,   796,  1438,   198,   220,   220,   220,\n",
       "                220,   220,   220,   220,  2116,    13, 19849,   796,\n",
       "               2746,   198,   220,   220,   220,   220,   220,   220,\n",
       "                220,  2116,    13, 46265, 22046,   796,   479,    86,\n",
       "              22046,   198,   220,   220,   220,   220,   220,   220,\n",
       "                220,  2116,    13,  3672,    62, 40290,   796,   705,\n",
       "              19849,     6,   198,   220,   220,   220,   220,   220,\n",
       "                220,   220,  2116,    13,  3672,    62, 37333,   844,\n",
       "                796,   705, 19849,    62,  3672,     6,   198,   220,\n",
       "                220,   220,   220,   220,   220,   220,  2116,    13,\n",
       "               3672,    62, 40290,    62, 40290,   796,   705, 19849,\n",
       "                 62,  3672,    62, 40290,     6,   198,   220,   220,\n",
       "                220,   220,   220,   220,   220,  2116,    13,  3672,\n",
       "                 62, 37333,   844,    62, 40290,   796,   705, 19849,\n",
       "                 62,  3672,    62, 37333,   844,     6,   198,   220,\n",
       "                220,   220,   220,   220,   220,   220,  2116,    13]],            dtype=int32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f6c746a-2d56-4da4-acb5-e066a6a230f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "class Model(nn.Module):\n",
      "    def __init__(self, name, model, **kwargs):\n",
      "        super(Model, self).__init__(name, model, **kwargs)\n",
      "        self.name = name\n",
      "        self.model = model\n",
      "        self.kwargs = kwargs\n",
      "        self.name_prefix ='model'\n",
      "        self.name_suffix ='model_name'\n",
      "        self.name_prefix_prefix ='model_name_prefix'\n",
      "        self.name_suffix_prefix ='model_name_suffix'\n",
      "        self.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8dba40-50b9-4268-95bf-c9ee5ef45ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
